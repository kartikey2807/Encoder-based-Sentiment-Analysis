{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11267687,"sourceType":"datasetVersion","datasetId":7043333},{"sourceId":319735,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":269788,"modelId":290778}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kartikeysharmaah/1tr720-notebook-3?scriptVersionId=231927593\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Fine-tuning (for sentiment analysis)\n* TPU utilization discardedðŸ˜­\n* load model weights\n* load token bag\n* load train dataset\n* load valid dataset\n* remove the last mlp block\n* add another linear layer and sigmoid\n* add dropout (optional)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset and token**","metadata":{}},{"cell_type":"code","source":"class SentimentDataset(Dataset): ## Custom dataset copy\n    def __init__(self, text, target):\n        super().__init__()\n        self.text = text\n        self.target = target\n    def __len__(self):\n        return len(self.target)\n    def __getitem__(self,idx):\n        return self.text[idx],self.target[idx]\n\ntrain_dataset = torch.load('/kaggle/input/twitter-text-dataset/train_dataset.pt')\nvalid_dataset = torch.load('/kaggle/input/twitter-text-dataset/valid_dataset.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Train dataset size: {len(train_dataset)}')\nprint(f'Valid dataset size: {len(valid_dataset)}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_bag = torch.load('/kaggle/input/twitter-text-dataset/token_bag.pt')\ntoken_length = len(token_bag)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model weights**   \nDefine custom model class (redundancy)","metadata":{}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module): ## Multi-head\n    def __init__(self, embedding_dimension, num_heads):\n        super().__init__()\n        assert embedding_dimension % num_heads == 0\n\n        self.dim = embedding_dimension\n        self.num_heads = num_heads\n        self.head_dim = embedding_dimension//num_heads\n\n        self.K = nn.Linear(self.dim,self.dim)\n        self.V = nn.Linear(self.dim,self.dim)\n        self.Q = nn.Linear(self.dim,self.dim)\n        self.projection = nn.Linear(self.dim,self.dim,bias=False)\n\n    def forward(self, x):\n\n        B,N,D = x.shape\n\n        Kx = self.K(x) ## BxNxD\n        Qx = self.Q(x)\n        Vx = self.V(x)\n\n        Kx = torch.reshape(Kx,(B,N,self.num_heads,self.head_dim)) ## BxNxHx(D/H)\n        Qx = torch.reshape(Qx,(B,N,self.num_heads,self.head_dim))\n        Vx = torch.reshape(Vx,(B,N,self.num_heads,self.head_dim))\n\n        Attx = nn.Softmax(dim=3)((1/np.sqrt(self.head_dim))*torch.transpose(Qx,1,2)@torch.transpose(torch.transpose(Kx,1,2),2,3)) ## BxHxNxN\n        Satx = torch.transpose(Attx@torch.transpose(Vx,1,2),1, 2) ## BxNxHx(D/H)\n\n        return self.projection(torch.reshape(Satx,(B,N,self.dim)))## BxNxD","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, input_dimension):\n        super().__init__()\n        self.dim = input_dimension\n\n        self.gelu = torch.nn.GELU(approximate='tanh')\n        self.l1 = nn.Linear(self.dim,self.dim*4)\n        self.l2 = nn.Linear(self.dim*4,self.dim)\n\n    def forward(self, x):\n        return self.l2(self.gelu(self.l1(x))) ## MLP!","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, embedding_dimension, num_heads):\n        super().__init__()\n        \n        self.dim = embedding_dimension\n        self.num_heads = num_heads\n\n        self.mhsa = MultiHeadSelfAttention(self.dim,self.num_heads)\n        self.ln1  = nn.LayerNorm(self.dim)\n        self.ln2  = nn.LayerNorm(self.dim)\n        self.mlp  = MLP(self.dim)\n\n    def forward(self, x):\n        y1 = self.mhsa(x)\n        y2 = x + y1\n        y3 = self.ln1(y2)\n        y4 = self.mlp(y3)\n        y5 = y3 + y4\n        y6 = self.ln2(y5)\n        return y6 ## mhsa -> ln1->mlp->ln2","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, embedding_dimension, num_heads, label, tweet_length=400):\n        super().__init__()\n\n        ## Encoder block contains 2 transformer layers,\n        ## followed by a linear layer that outputs one \n        ## Inputs have tokens and postional embeddings.\n        \n        self.dim = embedding_dimension\n        self.num_heads = num_heads\n        self.label = label\n\n        self.t1 = Transformer(self.dim,self.num_heads)\n        self.t2 = Transformer(self.dim,self.num_heads)\n        self.te = nn.Embedding(token_length, self.dim)\n        self.pe = nn.Embedding(tweet_length, self.dim)\n\n        self.pipelines = nn.Sequential(nn.Linear(self.dim,4*self.dim),nn.ReLU(),\n                                       nn.Linear(4*self.dim,token_length))\n\n    def forward(self, x):\n\n        N = x.shape[1]\n        embed = self.te(x) + self.pe(torch.arange(N,device=x.device))\n        embed = self.t1(embed)\n        embed = self.t2(embed)\n        embed = self.pipelines(embed)\n        return embed","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## HYPERPARAMETERS\nembedding_dimension = 256\nnum_heads = 8\nepochs = 3\nlearning_rate = 0.001","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_model = Encoder(embedding_dimension,num_heads,1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## save of GPU, load on CPU\n## CHANGE\nencoder_model.load_state_dict(torch.load('/kaggle/input/bert-encoder-model/pytorch/alpha/1/encoder_model.pth',map_location=device,weights_only=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for layer in encoder_model.state_dict():\n    print(f'Layer: {layer}')\n    print(f'Shape: {encoder_model.state_dict()[layer].shape}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Need to drop 'pipeline' layers**","metadata":{}},{"cell_type":"code","source":"encoder_model.pipelines = nn.Identity() ## identity takes and sends the same value","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"## Create module class that takes output from Encoder\n## model and pass it into a the linear layer. Finally\n## add the  sigmoid function. We put sigmoid on first\n## embedding which is the <cls> tag. Threshold is 0.5\n\nclass UpdatedEncoder(nn.Module):\n    def __init__(self, model, embedding_dimension):\n        super().__init__()\n        self.model = model\n        self.dim   = embedding_dimension\n\n        self.stack = nn.Sequential(nn.Linear(self.dim,4*self.dim),nn.ReLU(),nn.Dropout(0.3),\n                                   nn.Linear(4*self.dim,1)) ## single logit\n    def forward(self, x):\n        return self.stack(self.model(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"updated_encoder_model = UpdatedEncoder(encoder_model, embedding_dimension).to(device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for layer in updated_encoder_model.state_dict():\n    print(f'Layer: {layer}')\n    print(f'Shape: {updated_encoder_model.state_dict()[layer].shape}') ## gets model overview","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**test model**","metadata":{}},{"cell_type":"code","source":"tempx = torch.tensor([[1,345,4324,66,46930,8665,637,2,9748,10]],device=device)\ntempy = updated_encoder_model(tempx)[0,0,0] ## considering only <cls> gets tag\nprint(tempy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.SGD(updated_encoder_model.parameters(),lr=learning_rate,momentum=0.9)\nloss = nn.BCEWithLogitsLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"def text_embedding(text):\n    embedding = []\n    token_list = str(text).lower().split(' ')\n    for token in token_list:\n        \n        token = token.strip().strip('.')\n        token = token.strip()\n        \n        if len(token) == 0:\n            continue\n        if token[0] == '@':\n            continue\n\n        yoken = ''\n        for chars in token:\n            if chars =='.':\n                if yoken in token_bag.keys():\n                    embedding.append(token_bag[yoken])\n            else:\n                yoken = yoken + chars\n        if yoken in token_bag.keys():\n            embedding.append(token_bag[yoken])\n\n    return torch.tensor([embedding])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"updated_encoder_model.train()\n## fine-tuning: train model\nfor epoch in range(epochs):\n    step = 0\n    loss_avg = []\n    for text,target in train_dataset:\n        x = text_embedding(text)\n        ## add <cls> tag to the\n        ## text, that maps to 1\n        \n        if x.shape[1] == 0:\n            continue\n\n        x = torch.cat((torch.tensor([[1]]),x),dim=1)\n        y = torch.tensor(target,dtype=float)\n\n        ## Training\n        optimizer.zero_grad() ## set gradients to 0\n        x = x.to(device=device)\n        y = y.to(device=device)\n        logits = updated_encoder_model(x)[0,0,0]\n        loss_value = loss(logits,y)\n        loss_avg.append(loss_value)\n        \n        loss_value.backward()\n        optimizer.step()\n\n        if step%10000 == 0:\n            loss_mean = torch.mean(torch.tensor(loss_avg)) ## ways to capture progress\n            print(f'Epochs: {epoch+1} \\t Step: {step+1} \\t Mean Loss: {loss_mean:.2f}')\n            loss_avg = []\n        step += 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}}]}